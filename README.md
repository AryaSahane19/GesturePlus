# GesturePlus
GesturePlus aims to bridge the gap between gesture-based and traditional input methods, promoting  wider adoption of gesture-based interactions.

GesturePlus: A Hands-Free Revolution in Human-Computer Interaction 
GesturePlus is a pioneering system designed to transform the way we interact with computers. By harnessing the power of advanced machine learning and computer vision, GesturePlus offers a handsfree, intuitive interface that revolutionizes human-computer interaction. 
Breaking free from the constraints of traditional input methods, GesturePlus provides a versatile solution suitable for a wide range of environments, including those requiring sterility or accommodating users with limited mobility. Its tri-module approach—combining gesture control, voice commands, and a chatbot—ensures enhanced usability and real-time responsiveness, paving the way for a more inclusive and accessible digital experience.


Algorithm 
1.Gesture Recognition Algorithm 
The gesture recognition algorithm begins by capturing the video frames from the webcam using OpenCV. Each frame is instantly preprocessed, which includes horizontal  lipping and color-space conversion to effectively normalize the image for the upcoming processes. These preprocessed frames are then administered into the MediaPipe Hands model for robust detection of up to two-hands-per-frame, and for their 21 key landmarks to be obtained from each detected hand. On availability of landmarks, the algorithm uses Python's math library and numpy to calculate various distances and ratios from critical points, such as the  ingertip and corresponding joints. Such analysis allows the system to ascertain the states of either of the  ingers via ratio-based comparisons where a method like get_signed_dist()is used. The  inal  inger states, are encoded into a bitwise representation for comparison against a predeclared enumeration of gestures, such as FIST, PALM, V_GEST etc. To ensure reliability, the algorithm puts temporal smoothing in place, counting sustained gesture detections across a certain time frame before con irming a gesture. Upon the gesture being recognized with con idence, its control action, e.g., mouse cursor movement, clicking, and scrolling, is executed through pyautogui. Dependent on underlying OS, in some instances, the algorithm is designed to call libraries conditionally (e.g., osascript on macOS, alsaaudio with xrandr on Linux, or pycaw along with screen_brightness_control for Windows) for such system controls as volume or brightness, ensuring that every gesture will get the correct action pertinent to the OS that it's being run on.  

2. Voice Command Processing Algorithm 
The present design for the voice command processing algorithm accounts for swift real-time audio input handling. It starts off the SpeechThread, a special QThread responsible for an audio stream from the microphone. It is con igured in such a way that it adapts to the ambient noise level in real time so that it can still capture snippets of clear voices in less-than-desirable acoustic conditions. The audio is passed to SpeechRecognition, which converts the spoken words into corresponding texts by utilizing Google´s speech-to-text API, thus keeping variations in speech and accents in view. The recognized text is sent as signals to the main application thread, where the VoiceAssistant class takes over. In this class, a chain of conditional statements further parses the text to detect keywords and intents: e.g., "search", "open", "time", etc., or to activate the gesture-recognition module. Based on intent detection, corresponding actions will be invoked—an action such as conducting a web search, for example, opening an application, browsing through  ile directories, etc. The parsed text is also fed into pyttsx3, the TTS(Text-to-Speech) engine, so that the system can speak back responses and con irmations. Moreover, the algorithm is designed to incorporate features resilient to noise, which add thresholds to reading energies so that the algorithm can negotiate contextual environments by constructing a directory stack while ensuring that the commands of users are considered in the appropriate context. 

3. Parallel Processing 
To facilitate parallel processing for both the gesture recognition and voice command processing sub systems as done in the run_parallel.py module using multiprocessing library of Python. Basically, running two separate processes for gesture and voice modules allows them to co-run on their CPU cores without hindering each other. By isolating all the resource-consuming operations into independent processes, the system is freed from latency mechanisms and potential bottlenecks, thus continuing to provide excellent responsiveness. This parallel architecture increases system ef iciency considerably and also makes the debugging and maintenance much easier since each process can be handled alone. This results an advanced multi-modal interaction system on which real-time video analysis and audio processing operates seamlessly, without being a bother to each other. 

4. Resource Monitoring 
Resource monitoring forms a critical list in this entire system, which ensures that performance parameters like CPU performance, memory, and I/O operations are always tracked and analyzed. Achieved through the psutil library, this continues polling process-speci ic metrics at sample intervals of 1 sec. The algorithm looks at successive differences in I/O counter readings to calculate throughput values like read and write speeds. Through these delta calculations, we can see time-varying performance of the system. The collected data are put into time-series data sets visualized by Plotly, producing interactive charts for a deeper analysis of resource utilization patterns. Not only will it provide real-time feedback for diagnosing performance issues, but it will also keep the system stable and responsive at all times, while serving heavy loads and collect signi icant data to optimize and maintain the system over time. 
